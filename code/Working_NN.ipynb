{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "import h5py\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "global_variables = {\n",
    "    'epsilon':1e-5,\n",
    "    'leaky_relu_alpha':1e-3,\n",
    "    'train_rate': 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shuffle_index(array):\n",
    "    # Returns a train index for a dataset\n",
    "    train_ix = np.random.choice(range(len(array)), replace=False, size=int(len(array)))\n",
    "    return train_ix\n",
    "    \n",
    "def get_indices(array, index):\n",
    "    # slices an array along an index\n",
    "    return array[index]\n",
    "\n",
    "def shuffle(arrays):\n",
    "    '''\n",
    "    shuffles an array along its primary axis then retains that index for other datasets\n",
    "    \n",
    "    arrays - a list of arrays that should be shuffled in the same way, e.g X variables and corresponding features\n",
    "    '''\n",
    "    \n",
    "    indices = get_shuffle_index(arrays[0])\n",
    "    return [get_indices(array,indices) for array in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NON-CLASS FUNCTION LIBRARU\n",
    "\n",
    "def layermult_forward(X, W, b):\n",
    "    #forward pass for weight-input-bias calculation\n",
    "    out = X @ W + b\n",
    "    for_backprop = (W, X)\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def layermult_backward(dout, for_backprop):\n",
    "    #backprop for weight-input-bias calculation\n",
    "    W, h = for_backprop\n",
    "\n",
    "    dW = h.T @ dout\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dX = dout @ W.T\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "def softmax(X):\n",
    "    #forward pass for softmax activation\n",
    "    eX = np.exp((X.T - np.max(X, axis=1)).T)\n",
    "    return (eX.T / eX.sum(axis=1)).T\n",
    "\n",
    "def relu_forward(X):\n",
    "    #forward pass for ReLU activation\n",
    "    out = np.maximum(X, 0)\n",
    "    for_backprop = X\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def relu_backward(dout, for_backprop):\n",
    "    #backprop for ReLU activation\n",
    "    dX = dout.copy()\n",
    "    dX[for_backprop <= 0] = 0\n",
    "    return dX\n",
    "\n",
    "\n",
    "def lrelu_forward(X):\n",
    "    # Forward pass for leaky ReLU\n",
    "    out = np.maximum(global_variables['leaky_relu_alpha'] * X, X)\n",
    "    for_backprop = (X, global_variables['leaky_relu_alpha'])\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def lrelu_backward(dout, for_backprop):\n",
    "    # backward pass for leaky ReLU activation\n",
    "    X = for_backprop\n",
    "    dX = dout.copy()\n",
    "    dX[X < 0] *=  global_variables['epsilon']\n",
    "    return dX\n",
    "\n",
    "\n",
    "def sigmoid_forward(X):\n",
    "    # forward pass for sigmoid activation\n",
    "    out = np.sigmoid(X)\n",
    "    for_backprop = out\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def sigmoid_backward(dout, for_backprop):\n",
    "    # backward pass for sigmoid activation\n",
    "    return for_backprop * (1. - for_backprop) * dout\n",
    "\n",
    "\n",
    "def tanh_forward(X):\n",
    "    # forward pass for tanh activation\n",
    "    out = np.tanh(X)\n",
    "    for_backprop = out\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def tanh_backward(dout, for_backprop):\n",
    "    # backward pass for tanh activation\n",
    "    dX = (1 - for_backprop**2) * dout\n",
    "    return dX\n",
    "\n",
    "\n",
    "def dropout_forward(X, p_dropout):\n",
    "    # forward pass for dropout, switching off nodes\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    for_backprop = u\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def dropout_backward(dout, for_backprop):\n",
    "    # backward pass for dropout, carrying over gradients\n",
    "    dX = dout * for_backprop\n",
    "    return dX\n",
    "\n",
    "\n",
    "def bn_forward(X, gamma, beta, for_backprop, momentum=.9, train=True):\n",
    "    # Batch normalisation forward pass, storing information for backprop later\n",
    "    rmean, rvar = for_backprop\n",
    "\n",
    "    if train:\n",
    "        mu = np.mean(X, axis=0)\n",
    "        var = np.var(X, axis=0)\n",
    "\n",
    "        X_norm = (X - mu) / np.sqrt(var + global_variables['epsilon'])\n",
    "        out = gamma * X_norm + beta\n",
    "\n",
    "        for_backprop = (X, X_norm, mu, var, gamma, beta)\n",
    "\n",
    "        rmean = exp_running_avg(rmean, mu, momentum)\n",
    "        rvar = exp_running_avg(rvar, var, momentum)\n",
    "    else:\n",
    "        X_norm = (X - rmean) / np.sqrt(rvar + global_variables['epsilon'])\n",
    "        out = gamma * X_norm + beta\n",
    "        for_backprop = None\n",
    "\n",
    "    return out, for_backprop, rmean, rvar\n",
    "\n",
    "def exp_running_avg(moving, new, gamma=.9):\n",
    "    #exponential decay function for the moving average\n",
    "    return gamma * moving + (1. - gamma) * new\n",
    "\n",
    "def bn_backward(dout, for_backprop):\n",
    "    #Pass the gradient through batch normalisation, retaining the derivatives of gamma and beta\n",
    "    X, X_norm, mu, var, gamma, beta = for_backprop\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    X_mu = X - mu\n",
    "    std_inv = 1. / np.sqrt(var + global_variables['epsilon'])\n",
    "\n",
    "    dX_norm = dout * gamma\n",
    "    dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n",
    "    dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "    dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n",
    "    dgamma = np.sum(dout * X_norm, axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "    return dX, dgamma, dbeta\n",
    "\n",
    "def init_weight(dim1,dim2):\n",
    "    # initialise weights for parameters, using their square root to push them out to 1\n",
    "    return np.random.randn(dim1, dim2) / np.sqrt(dim1)\n",
    "\n",
    "def init_bias(dim1):\n",
    "    #initialise bias parameter for node multiplication (also betas for batch normalisation)\n",
    "    return np.zeros((1, dim1))\n",
    "\n",
    "def init_gamma(dim1):\n",
    "    #initialise gamma parameter for batch normalisation\n",
    "    return np.ones((1,dim1))\n",
    "\n",
    "def cel(y_pred, y_train):\n",
    "    #Computes Cross Entropy Loss\n",
    "    m = len(y_pred)\n",
    "    prob = softmax(y_pred)\n",
    "    log_like = -np.log(prob[range(m), y_train])\n",
    "    return np.sum(log_like) / m\n",
    "\n",
    "def d_cel(y_pred, y_train):\n",
    "    #Derivative for Cross Entropy Loss\n",
    "    m = y_pred.shape[0]\n",
    "    grad_y = softmax(y_pred)\n",
    "    grad_y[range(m), y_train] -= 1.\n",
    "    grad_y /= m\n",
    "    return grad_y\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    #Score the average accuracy for a prediction against the ground truth\n",
    "    return (y_pred == y_true).mean()\n",
    "\n",
    "def get_minibatch(X, y, batch_size, shuffled=True):\n",
    "    '''\n",
    "    Given features and a label, randomly sample the rows and create multiple batches, keeping indices\n",
    "    consistent between datasets\n",
    "    \n",
    "    X - feature data\n",
    "    y - ground truth\n",
    "    batch_size - number of rows per batch\n",
    "    '''\n",
    "    batch_list = []\n",
    "\n",
    "    if shuffled:\n",
    "        X, y = shuffle([X, y])\n",
    "\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        X_batch = X[i:i + batch_size]\n",
    "        y_batch = y[i:i + batch_size]\n",
    "        batch_list.append((X_batch, y_batch))\n",
    "\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterconNet:\n",
    "        \n",
    "    def __init__(self, n_features, n_classes, hidden_layer_nodes, p_dropout=.8):\n",
    "        self.init_params(n_features, n_classes, hidden_layer_nodes)\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mode = 'classification'\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        A single training iteration over some data (usually a minibatch). Does forward propagation, calculates\n",
    "        loss and propagates gradients back through the newtork. Takes a training and test set of equal length\n",
    "        and index\n",
    "        \"\"\"\n",
    "        y_pred, carry_over = self.forward(X_train, train=True)\n",
    "        loss = cel(y_pred, y_train)\n",
    "        grad = self.backpropagation(y_pred, y_train, carry_over)\n",
    "\n",
    "        return grad, loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        #Predict the probability that a given observation is in a given class, over an input set\n",
    "        score, _ = self.forward(X, False)\n",
    "        return softmax(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        #Predict the class of each observation, given an input set\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def forward(self, X, train=False):\n",
    "        '''\n",
    "        Run through the forward-propagation sequence for the neural network, ensuring necessary information for\n",
    "        calculating backpropagation is stored in a carry_over dictionary.\n",
    "        '''\n",
    "        gamma1, gamma2 = self.params['gamma1'], self.params['gamma2']\n",
    "        beta1, beta2 = self.params['beta1'], self.params['beta2']\n",
    "\n",
    "        u1, u2 = None, None\n",
    "        bn1_carry_over, bn2_carry_over = None, None\n",
    "\n",
    "        # First layer\n",
    "        h1, h1_carry_over = layermult_forward(X, self.params['W1'], self.params['b1'])\n",
    "        bn1_carry_over = (self.bn_carry_overs['bn1_ave'], self.bn_carry_overs['bn1_var'])\n",
    "        h1, bn1_carry_over, run_mean, run_var = bn_forward(h1, gamma1, beta1, bn1_carry_over, train=train)\n",
    "        h1, nl_carry_over1 = relu_forward(h1)\n",
    "\n",
    "        self.bn_carry_overs['bn1_ave'], self.bn_carry_overs['bn1_var'] = run_mean, run_var\n",
    "\n",
    "        if train:\n",
    "            h1, u1 = dropout_forward(h1, self.p_dropout)\n",
    "\n",
    "        # Second layer\n",
    "        h2, h2_carry_over = layermult_forward(h1, self.params['W2'], self.params['b2'])\n",
    "        bn2_carry_over = (self.bn_carry_overs['bn2_ave'], self.bn_carry_overs['bn2_var'])\n",
    "        h2, bn2_carry_over, run_mean, run_var = bn_forward(h2, gamma2, beta2, bn2_carry_over, train=train)\n",
    "        h2, nl_carry_over2 = relu_forward(h2)\n",
    "\n",
    "        self.bn_carry_overs['bn2_ave'], self.bn_carry_overs['bn2_var'] = run_mean, run_var\n",
    "\n",
    "        if train:\n",
    "            h2, u2 = dropout_forward(h2, self.p_dropout)\n",
    "\n",
    "        # Third layer\n",
    "        score, score_carry_over = layermult_forward(h2, self.params['W3'], self.params['b3'])\n",
    "\n",
    "        carry_over = (X, h1_carry_over, h2_carry_over, \n",
    "                      score_carry_over, nl_carry_over1, nl_carry_over2, \n",
    "                      u1, u2, bn1_carry_over, bn2_carry_over)\n",
    "\n",
    "        return score, carry_over\n",
    "\n",
    "    def backpropagation(self, y_pred, y_train, carry_over):\n",
    "        '''\n",
    "        Assuming a forward pass has been completed, propagate the gradients and error back through the network\n",
    "        '''\n",
    "        (X, h1_carry_over, h2_carry_over, \n",
    "         score_carry_over, nl_carry_over1, nl_carry_over2, \n",
    "         u1, u2, bn1_carry_over, bn2_carry_over) = carry_over\n",
    "\n",
    "        # Hidden layer 3\n",
    "        to_pass = d_cel(y_pred, y_train)\n",
    "        to_pass, d_weights_3, d_bias_3 = layermult_backward(to_pass, score_carry_over)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        to_pass = relu_backward(to_pass, nl_carry_over2)\n",
    "        to_pass = dropout_backward(to_pass, u2)\n",
    "        to_pass, dgamma2, dbeta2 = bn_backward(to_pass, bn2_carry_over)\n",
    "        to_pass, d_weights_2, d_bias_2 = layermult_backward(to_pass, h2_carry_over)\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        to_pass = relu_backward(to_pass, nl_carry_over1)\n",
    "        to_pass = dropout_backward(to_pass, u1)\n",
    "        to_pass, dgamma1, dbeta1 = bn_backward(to_pass, bn1_carry_over)\n",
    "        _, d_weights_1, d_bias_1 = layermult_backward(to_pass, h1_carry_over)\n",
    "\n",
    "        gradients = dict(\n",
    "            W1=d_weights_1, W2=d_weights_2, W3=d_weights_3, \n",
    "            b1=d_bias_1, b2=d_bias_2, b3=d_bias_3, \n",
    "            gamma1=dgamma1,gamma2=dgamma2, beta1=dbeta1, beta2=dbeta2\n",
    "        )\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    \n",
    "    def init_params(self, D, C, H):\n",
    "        self.params = dict(\n",
    "            W1=init_weight(D, H),W2=init_weight(H, H),W3=init_weight(H, C), #initialise layer weights\n",
    "            b1=init_bias(H),b2=init_bias(H),b3=init_bias(C), # initialise layer biases\n",
    "            gamma1=init_gamma(H),gamma2=init_gamma(H), # initialise gammas for batch norm\n",
    "            beta1=init_bias(H),beta2=init_bias(H) # initialise betas for batch norm\n",
    "        )\n",
    "\n",
    "        # initialise the batch-normalization gammas and betas\n",
    "        self.bn_carry_overs = dict(zip(['bn{}_{}'.format(i,j) for i in ['1','2'] for j in ['ave','var']],\n",
    "                 [init_bias(H) for i in range(4)]\n",
    "                )) \n",
    "        \n",
    "    def momentum_sgd(self, X_train, y_train, test=None, train_rate=1e-3, b_size=256, iterations=2000, p_iter=100):\n",
    "        '''\n",
    "        Initiate a training sequence via minibatched stochastic gradient descent. Returns a new version of the\n",
    "        network with updated parameter dictionary, but training is stateful just as it is in scikit-learn\n",
    "        \n",
    "        #Inputs:\n",
    "        X_train - The training data for the model\n",
    "        y_train - the ground truth\n",
    "        test - A tuple containing a validation training set and matched ground truth\n",
    "        train_rate - magnitude of parameter update\n",
    "        b_size - set the size of the minibatches to be used\n",
    "        iterations - stop training after this many iterations\n",
    "        p_iter - updates will be printed on multiples of this number\n",
    "        '''\n",
    "        momentum_gamma = .9 # set the parameter for momentum\n",
    "        velocity = {k: np.zeros_like(v) for k, v in self.params.items()} # initialise velocity as zero-vectors\n",
    "        minibatches = get_minibatch(X_train, y_train, b_size) # create the set of minibatches to be trained on\n",
    "\n",
    "        #Provide updates by scoring on a validation set, else score on the train sets\n",
    "        if test:\n",
    "            X_val, y_val = test\n",
    "        else:\n",
    "            X_val, y_val = X_train, y_train\n",
    "\n",
    "        for iteration in range(1, iterations + 1):\n",
    "            idx = np.random.randint(0, len(minibatches)) # select a random minibatch to train on\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "            gradients, loss = self.train(X_mini, y_mini) # \n",
    "\n",
    "            if iteration % p_iter == 0:\n",
    "                val_acc = accuracy(y_val, self.predict(X_val))\n",
    "                print('Iteration: {} loss: {:.4f} validation: {:4f}'.format(iteration, loss, val_acc))\n",
    "\n",
    "            for param in gradients:\n",
    "                velocity[param] = momentum_gamma * velocity[param] + train_rate * gradients[param]\n",
    "                self.params[param] -= velocity[param]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,Y = shuffle([data,label])\n",
    "X_train = data[:40000]\n",
    "Y_train = label[:40000]\n",
    "X_test = data[40000:50000]\n",
    "Y_test = label[40000:50000]\n",
    "X_val = data[50000:]\n",
    "Y_val = label[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200 loss: 2.5483 validation: 0.631000\n",
      "Iteration: 400 loss: 2.2788 validation: 0.664200\n",
      "Iteration: 600 loss: 2.1991 validation: 0.628900\n",
      "Iteration: 800 loss: 2.1749 validation: 0.551100\n",
      "Iteration: 1000 loss: 2.1646 validation: 0.583800\n",
      "CPU times: user 1min 7s, sys: 6.61 s, total: 1min 14s\n",
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(10)\n",
    "\n",
    "n_features = 128\n",
    "n_classes = 10\n",
    "hidden_layer_width = 500\n",
    "p_dropout=0.05\n",
    "\n",
    "t = InterconNet(n_features, n_classes, hidden_layer_width, p_dropout=p_dropout)\n",
    "t.momentum_sgd(X_train, Y_train, test=(X_val, Y_val), b_size=300, iterations=1000, p_iter=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
