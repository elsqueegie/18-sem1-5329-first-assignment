{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "import h5py\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "data = data.copy()\n",
    "data = np.c_[data, np.ones(len(data))]\n",
    "weights = (np.random.rand(g.shape[1]) - 0.5)/100\n",
    "alphas = (np.random.rand(g.shape[1]) - 0.5)/100\n",
    "\n",
    "g = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(column):\n",
    "    # activation function that gives x if x > 0 else 0\n",
    "    return np.max(np.array([np.zeros((len(column),)),column]),axis=0)\n",
    "\n",
    "def leaky_relu(column, alpha=0.05):\n",
    "    # activation function that draws on ReLU but has a slight gradient for x < 0\n",
    "    return np.max(np.array([column*alpha,column]),axis=0)\n",
    "\n",
    "def sigmoid(column):\n",
    "    # activation function that returns a value between 0 and 1, good for probabilities\n",
    "    return 1/(1+np.exp(-column))\n",
    "\n",
    "def tanh(column):\n",
    "    # Activation function that returns a value between -1 and 1\n",
    "    # problems occurred with large negatives when applying (1 - np.exp(-column)) / (1 + np.exp(-column)) \n",
    "    # using numpy equivalent instead\n",
    "    return np.tanh(column)\n",
    "\n",
    "def node_mult(in_data, weights):\n",
    "    # Multiply each feature (including constant) by its weight then sum the result\n",
    "    in_data = in_data.copy()\n",
    "    for d in range(in_data.shape[1]):\n",
    "        in_data[:,d] = weights[d] * in_data[:,d]\n",
    "    return in_data.sum(axis=1)\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    #returns the gradient of a sigmoid at point x\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def grad_tanh(x):\n",
    "    #returns the gradient of a tanh at point x\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def grad_relu(x):\n",
    "    #returns the gradient of a relu at point x\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def grad_leaky_relu(x, alpha=0.05):\n",
    "    #returns the gradient of a leaky_relu at point x\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return alpha\n",
    "    \n",
    "def grad_softmax(prediction_list, iclass):\n",
    "    j = np.exp(prediction_list)\n",
    "    i = j\n",
    "    \n",
    "def get_gradient(activation_function):\n",
    "    gradient_dic = {\n",
    "        'relu':grad_relu,\n",
    "        'leaky_relu':grad_leaky_relu,\n",
    "        'tanh':grad_tanh,\n",
    "        'sigmoid':grad_sigmoid\n",
    "    }\n",
    "    return gradient_dic[activation_function]\n",
    "\n",
    "def activate(in_data, kind='relu'):\n",
    "    # Apply an activation function to a node's output\n",
    "    actionary = {\n",
    "        'relu':relu,\n",
    "        'leaky_relu':leaky_relu,\n",
    "        'sigmoid':sigmoid,\n",
    "        'tanh':tanh\n",
    "    }\n",
    "    return actionary[kind](in_data)\n",
    "\n",
    "def compute_softmax_score(prediction_list, iclass):\n",
    "    j = np.exp(prediction_list)\n",
    "    j = j/j.sum()\n",
    "    return j[iclass] / j.sum()\n",
    "    \n",
    "def compute_cross_entropy_loss(yhat):\n",
    "    return 0 - np.log(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    '''\n",
    "    A Node is a modular element of a neural network. It is defined by:\n",
    "    \n",
    "    in_data - the feature inputs, including a constant feature\n",
    "    activation - the activation function to be applied to the node output\n",
    "    weights - the coefficients to be applied to the features positionally\n",
    "    train_rate - the rate at which gradient descent updates the weights of the node\n",
    "    max_iter - the number of training steps to take before ending a training session\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 activation='sigmoid', \n",
    "                 weights='None', \n",
    "                 train_rate=0.01, \n",
    "                 max_iter=1000):\n",
    "        self.n_features = n_features\n",
    "        if weights=='None':\n",
    "            self.weights=np.random.rand(n_features) / 50\n",
    "        else:\n",
    "            self.weights=weights\n",
    "        self.activation_func = activation\n",
    "        self.train_rate = train_rate\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def set_input(self,in_data):\n",
    "        # Set the input data for the node, must have number of features equal to n_features used for initialisation\n",
    "        self.in_data = in_data\n",
    "        \n",
    "    def score_input(self, in_data, weights='None', alphas='None'):\n",
    "        # Apply the weights to the features and return the output for the data set in set_input\n",
    "        if weights=='None':\n",
    "            weights = self.weights\n",
    "        self.in_data = in_data.copy()\n",
    "        #print(self.in_data.shape)\n",
    "        self.output = activate(node_mult(self.in_data, weights),kind=self.activation_func)\n",
    "        #print(self.output.shape)\n",
    "        return self.output\n",
    "    \n",
    "    def score_gradients(self,increment=1e-4):\n",
    "        # Calculate the average gradient for each coefficient via a tiny increment over the input data\n",
    "        self.grads_ = []\n",
    "        for i in range(self.in_data.shape[1]):\n",
    "            self.new_weights = self.weights.copy()\n",
    "            self.new_weights[i] = self.new_weights[i] + increment\n",
    "            j1 = self.score_input()\n",
    "            j2 = self.score_input(weights=self.new_weights)\n",
    "            self.grads_.append(np.mean(j1-j2)/increment)\n",
    "        self.grads_ = np.array(self.grads_)\n",
    "        return self.grads_\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        afunc = self.activation\n",
    "        gradfunc = get_gradient(afunc)\n",
    "        for i in \n",
    "        \n",
    "            \n",
    "    def update_weights(self):\n",
    "        # update the coefficients in the direction of the gradient\n",
    "        # TODO - set to update against direction of the error when error calculation is done\n",
    "        self.score_gradients()\n",
    "        self.weights = self.weights + self.grads_ * self.train_rate\n",
    "        return self.weights\n",
    "        \n",
    "    def gradient_descend(self):\n",
    "        # recompute coefficients until gradients flatten or max_iter is reached\n",
    "        old = self.weights[:]\n",
    "        for i in range(self.max_iter):\n",
    "            new = self.update_weights()\n",
    "            if i % 200 == 0:\n",
    "                print(np.max(old - new))\n",
    "            if np.max(old - new) < 0.0000001:\n",
    "                break\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Layer:\n",
    "    \n",
    "    def __init__(self, n_nodes, activation, n_inputs):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.nodes = {}\n",
    "        self.activation = activation\n",
    "        self.n_inputs = n_inputs\n",
    "        afunc = self.activation\n",
    "        self.nodes = {i:Node(n_inputs, activation=afunc) for i in range(self.n_nodes)}\n",
    "        \n",
    "    def get_layer_output(self, df_in):\n",
    "        print(df_in.shape)\n",
    "        self.output = np.array([n.score_input(df_in) for n in self.nodes.values()])\n",
    "        return self.output\n",
    "        \n",
    "    \n",
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = {}\n",
    "        self.in_data = None\n",
    "        self.n_layers = 0\n",
    "        \n",
    "    def set_indata(self, in_data, label):\n",
    "        self.in_data = in_data\n",
    "        self.label = label\n",
    "        self.in_features = in_data.shape[1]\n",
    "        self.to_pass = self.in_data\n",
    "    \n",
    "    def assign_layer(self, n_nodes, activation, n_inputs):\n",
    "        \n",
    "        self.layers[self.n_layers] = Layer(n_nodes, activation, n_inputs)\n",
    "        #self.layers[self.n_layers].set_input(self.to_pass)\n",
    "        self.to_pass = self.layers[self.n_layers].n_nodes\n",
    "        self.n_layers += 1\n",
    "        \n",
    "    def get_network_output(self):\n",
    "        data_in = self.in_data.copy()\n",
    "        for ilayer in self.layers.values():\n",
    "            data_in = ilayer.get_layer_output(data_in).T\n",
    "        return data_in\n",
    "        \n",
    "    def score_network(self):\n",
    "        self.output = self.get_network_output()\n",
    "        self.predict_class = np.array([[i for i,j in enumerate(k) if j==max(k)][0] for k in self.output ])\n",
    "        self.probs = np.array(\n",
    "                [compute_softmax_score(self.output[i],self.predict_class[i]) for i in range(len(self.output))]\n",
    "        )\n",
    "        \n",
    "    def get_loss(self):\n",
    "        m = (self.label == self.predict_class).astype(int)\n",
    "        self.loss = -(m*np.log(self.probs) + (1-m)*np.log(self.probs))\n",
    "        return self.loss\n",
    "    \n",
    "    def get_batch(self, frac=0.05):\n",
    "        return np.random.choice(range(len(self.in_data)), replace=False, size=int(len(self.in_data)*frac))\n",
    "    \n",
    "    def get_gradient(self, batch):\n",
    "        try:\n",
    "            loss = self.loss\n",
    "        except:\n",
    "            loss = self.get_loss\n",
    "        \n",
    "        data = self.data_in[batch]\n",
    "        \n",
    "        layers = list(range(len(self.n_layers)))\n",
    "        layers.reverse\n",
    "        \n",
    "        for layer in layers:\n",
    "            for node in layer:\n",
    "                None\n",
    "    \n",
    "    def train(self, input_data):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.070910954097621659"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [-3.44,1.16,-0.81,3.91]\n",
    "\n",
    "compute_cross_entropy_loss(compute_softmax_score(scores,3))\n",
    "#Should be 0.070910954097621659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = list(range(0,10))\n",
    "t.reverse()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Network()\n",
    "t.set_indata(g[:1000], label[:1000])\n",
    "t.assign_layer(10,'leaky_relu', 129)\n",
    "t.assign_layer(10,'leaky_relu', 10)\n",
    "t.assign_layer(10,'sigmoid', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 129)\n",
      "(1000, 10)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "t.score_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([589, 927, 951,  27, 288, 382, 443, 697, 688, 345, 983, 287, 598,\n",
       "       631, 705, 381, 802, 741, 238, 858, 385, 111, 487, 292,  44, 320,\n",
       "       993, 349, 356, 694, 229, 150, 187, 272, 407, 415, 269, 767, 418,\n",
       "       875, 976, 943,  43, 134, 330, 601, 930, 185, 590, 855])"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14999999999999999"
      ]
     },
     "execution_count": 975,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t.predict_class == t.label).astype(int).sum() / len(t.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time taken to perform 129 weight adjustments times 1000 for a single node: 3mins 32s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
