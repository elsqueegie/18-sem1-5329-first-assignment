{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "import h5py\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "data = data.copy()\n",
    "#data = np.c_[data, np.ones(len(data))]\n",
    "\n",
    "g = data.copy()\n",
    "\n",
    "weights = (np.random.rand(g.shape[1]) - 0.5)/100\n",
    "alphas = (np.random.rand(g.shape[1]) - 0.5)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_constant(data):\n",
    "    return np.c_[data, np.ones(len(data))]\n",
    "\n",
    "def relu(matrix):\n",
    "    # activation function that gives x if x > 0 else 0\n",
    "    return np.clip(matrix, 0, None)\n",
    "\n",
    "def leaky_relu(matrix, alpha=0.005):\n",
    "    # activation function that draws on ReLU but has a slight gradient for x < 0\n",
    "    matrix[matrix<0] = matrix[matrix<0] * alpha\n",
    "    return matrix\n",
    "\n",
    "def sigmoid(column):\n",
    "    # activation function that returns a value between 0 and 1, good for probabilities\n",
    "    return 1/(1+np.exp(-column))\n",
    "\n",
    "def tanh(column):\n",
    "    # Activation function that returns a value between -1 and 1\n",
    "    # problems occurred with large negatives when applying (1 - np.exp(-column)) / (1 + np.exp(-column)) \n",
    "    # using numpy equivalent instead\n",
    "    return np.tanh(column)\n",
    "\n",
    "def softmax(layer_output):\n",
    "    j = np.exp(layer_output)\n",
    "    j /= j.sum(axis=1).reshape(len(j),1)\n",
    "    return j\n",
    "\n",
    "#def node_softmax(in_data, weights):\n",
    "#    # Multiply each feature (including constant) by its weight then sum the result\n",
    "#    in_data = in_data.copy()\n",
    "#    for d in range(in_data.shape[1]):\n",
    "#        in_data[:,d] = np.exp(weights[d] * in_data[:,d])\n",
    "#    in_data = in_data / in_data.sum(axis=1)\n",
    "#    return in_data\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    #returns the gradient of a sigmoid at point x\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def grad_tanh(x):\n",
    "    #returns the gradient of a tanh at point x\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def grad_relu(x):\n",
    "    #returns the gradient of a relu at point x\n",
    "    return (np.array(x) > 0) - 0.0\n",
    "\n",
    "def grad_leaky_relu(x, alpha=0.005):\n",
    "    #returns the gradient of a leaky_relu at point x\n",
    "    to_ret = np.array(grad_relu(x))\n",
    "    to_ret[to_ret<=0] = alpha\n",
    "    return to_ret\n",
    "    \n",
    "def grad_softmax(prediction_list, iclass):\n",
    "    prediction_list /= np.max(prediction_list,axis=1, keepdims=True)\n",
    "    j = np.exp(prediction_list)\n",
    "    j =  j/j.sum()\n",
    "    yhat = j[iclass]\n",
    "    return yhat - 1\n",
    "    \n",
    "def get_gradient(activation_function):\n",
    "    gradient_dic = {\n",
    "        'relu':grad_relu,\n",
    "        'leaky_relu':grad_leaky_relu,\n",
    "        'tanh':grad_tanh,\n",
    "        'sigmoid':grad_sigmoid,\n",
    "        'softmax':grad_softmax\n",
    "    }\n",
    "    return gradient_dic[activation_function]\n",
    "\n",
    "def activate(in_data, kind='relu'):\n",
    "    # Apply an activation function to a node's output\n",
    "    actionary = {\n",
    "        'relu':relu,\n",
    "        'leaky_relu':leaky_relu,\n",
    "        'sigmoid':sigmoid,\n",
    "        'tanh':tanh,\n",
    "        'softmax':softmax\n",
    "    }\n",
    "    return actionary[kind](in_data)\n",
    "\n",
    "def compute_softmax_scores(layer_output):\n",
    "    j = np.exp(layer_output)\n",
    "    j /= j.sum(axis=1).reshape(len(j),1)\n",
    "    return j\n",
    "    \n",
    "def compute_cross_entropy_loss(yhat,label):\n",
    "    return 0 - np.log(yhat[:,label])[0]\n",
    "\n",
    "def get_cross_entropy_grads(output,label):\n",
    "    n_instances = len(label)\n",
    "    gradients_matrix = compute_softmax_scores(output)\n",
    "    gradients_matrix[range(n_instances),label] -= 1\n",
    "    gradients_matrix /= n_instances\n",
    "    return gradients_matrix\n",
    "\n",
    "\n",
    "def node_mult(in_data, weights, softmax=False):\n",
    "    # Multiply each feature (including constant) by its weight then sum the result\n",
    "    in_data = add_constant(in_data)\n",
    "    if in_data.shape[1] != weights.shape[1]:\n",
    "        raise ValueError(\"Input matrix doesn't match weight vector: {} weights and {} features\".format(\n",
    "            len(weights),in_data.shape[1])\n",
    "        )\n",
    "    for d in range(in_data.shape[1]):\n",
    "        in_data[:,d] = weights[d] * in_data[:,d]\n",
    "    if softmax:\n",
    "        in_data[:,:-1] += in_data[:,-1:]\n",
    "        in_data = in_data[:,:-1]\n",
    "        in_data = np.exp(in_data)\n",
    "        return in_data/in_data.sum(axis=1).reshape((len(in_data),1))\n",
    "    else:\n",
    "        return in_data.sum(axis=1)\n",
    "    \n",
    "def layer_mult(in_data, weights, bias, activation_func='sigmoid'):\n",
    "    # Multiply each feature (including constant) by its weight then sum the result\n",
    "    #in_data = add_constant(in_data)\n",
    "    if in_data.shape[1] != weights.shape[0]:\n",
    "        raise ValueError(\"Input matrix doesn't match weight vector: {} weights and {} features\".format(\n",
    "            len(weights),in_data.shape[1])\n",
    "        )\n",
    "    if activation_func=='softmax':\n",
    "        layer_out = in_data.dot(weights) + bias\n",
    "        return softmax(layer_out)\n",
    "    else:\n",
    "        layer_out = in_data.dot(weights) + bias\n",
    "        return activate(layer_out, kind=activation_func).T\n",
    "    \n",
    "def matricise_label(label,output):\n",
    "    T = np.zeros_like(output)\n",
    "    T[range(len(T)),label] += 1\n",
    "    return T\n",
    "\n",
    "def get_cost(label,output):\n",
    "    label = matricise_label(label,output)\n",
    "    return -np.multiply(label,np.log(output)).sum()\n",
    "\n",
    "def error_output(label,output):\n",
    "    label = matricise_label(label,output)\n",
    "    return output - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    '''\n",
    "    A Node is a modular element of a neural network. It is defined by:\n",
    "    \n",
    "    in_data - the feature inputs, including a constant feature\n",
    "    activation - the activation function to be applied to the node output\n",
    "    weights - the coefficients to be applied to the features positionally\n",
    "    train_rate - the rate at which gradient descent updates the weights of the node\n",
    "    max_iter - the number of training steps to take before ending a training session\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 activation='sigmoid', \n",
    "                 weights='None', \n",
    "                 train_rate=0.01, \n",
    "                 max_iter=1000):\n",
    "        self.n_features = n_features\n",
    "        if weights=='None':\n",
    "            self.weights=np.random.rand(n_features+1) / 50\n",
    "        else:\n",
    "            self.weights=weights\n",
    "        self.activation_func = activation\n",
    "        self.train_rate = train_rate\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    #def set_input(self,in_data):\n",
    "        # Set the input data for the node, must have number of features equal to n_features + constant\n",
    "        #self.in_data = add_constant(in_data)\n",
    "        \n",
    "    def score_input(self, in_data, weights='None', alphas='None'):\n",
    "        # Apply the weights to the features and return the output for the data set in set_input\n",
    "        if weights=='None':\n",
    "            weights = self.weights\n",
    "        self.in_data = in_data\n",
    "        self.output = activate(node_mult(self.in_data, weights, self.activation_func=='softmax'),\n",
    "                               kind=self.activation_func)\n",
    "        return self.output\n",
    "    \n",
    "    def score_gradients(self,increment=1e-4):\n",
    "        # Calculate the average gradient for each coefficient via a tiny increment over the input data\n",
    "        self.grads_ = []\n",
    "        for i in range(self.in_data.shape[1]):\n",
    "            self.new_weights = self.weights.copy()\n",
    "            self.new_weights[i] = self.new_weights[i] + increment\n",
    "            j1 = self.score_input()\n",
    "            j2 = self.score_input(weights=self.new_weights)\n",
    "            self.grads_.append(np.mean(j1-j2)/increment)\n",
    "        self.grads_ = np.array(self.grads_)\n",
    "        return self.grads_\n",
    "        \n",
    "            \n",
    "    def update_weights(self):\n",
    "        # update the coefficients in the direction of the gradient\n",
    "        # TODO - set to update against direction of the error when error calculation is done\n",
    "        self.score_gradients()\n",
    "        self.weights = self.weights + self.grads_ * self.train_rate\n",
    "        return self.weights\n",
    "        \n",
    "    def gradient_descend(self):\n",
    "        # recompute coefficients until gradients flatten or max_iter is reached\n",
    "        old = self.weights[:]\n",
    "        for i in range(self.max_iter):\n",
    "            new = self.update_weights()\n",
    "            if i % 200 == 0:\n",
    "                print(np.max(old - new))\n",
    "            if np.max(old - new) < 0.0000001:\n",
    "                break\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, n_nodes, activation, n_inputs):\n",
    "        self.activation = activation\n",
    "        afunc = self.activation\n",
    "        self.weights = np.random.random((n_inputs,n_nodes))\n",
    "        self.bias = np.random.randn(n_nodes)\n",
    "        \n",
    "    def get_layer_output(self, df_in):\n",
    "        self.in_data = df_in\n",
    "        self.output = layer_mult(df_in, self.weights, self.bias, activation_func=self.activation)\n",
    "        return self.output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = {}\n",
    "        self.in_data = None\n",
    "        self.n_layers = 0\n",
    "        \n",
    "    def set_indata(self, in_data, label):\n",
    "        self.in_data = in_data\n",
    "        self.label = label\n",
    "        self.in_features = in_data.shape[1]\n",
    "        self.to_pass = self.in_data\n",
    "    \n",
    "    def assign_layer(self, n_nodes, activation, n_inputs):\n",
    "        self.layers[self.n_layers] = Layer(n_nodes, activation, n_inputs)\n",
    "        #self.layers[self.n_layers].set_input(self.to_pass)\n",
    "        self.n_layers += 1\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        self.outputs_by_layer = []\n",
    "        data_in = self.in_data.copy()\n",
    "        for ilayer in self.layers.values():\n",
    "            #print(ilayer.activation)\n",
    "            self.ilayer = ilayer\n",
    "            data_in = ilayer.get_layer_output(data_in).T\n",
    "            self.outputs_by_layer.append(data_in.copy())\n",
    "        self.output = data_in.reshape(data_in.shape[:2]).T\n",
    "        return self.outputs_by_layer\n",
    "\n",
    "    \n",
    "    def score_network(self):\n",
    "        self.feed_forward()\n",
    "        self.prediction = np.argmax(t.output, axis=1).reshape(len(t.label))\n",
    "        self.error = compute_cross_entropy_loss(self.output, self.label)\n",
    "\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return np.mean([compute_cross_entropy_loss(\n",
    "                compute_softmax_score(self.output[i],self.label[i])\n",
    "        ) for i in range(len(self.label))])\n",
    "    \n",
    "    def get_batched_loss(self,batch):\n",
    "        loss = [compute_cross_entropy_loss(compute_softmax_score(self.output[i], self.label[i])) for i in batch]\n",
    "        return loss\n",
    "    \n",
    "    def get_batch(self, frac=0.05):\n",
    "        return np.random.choice(range(len(self.in_data)), replace=False, size=int(len(self.in_data)*frac))\n",
    "\n",
    "    \n",
    "    def backpropagate(self):\n",
    "        cost = get_cost(self.label, self.output)\n",
    "        error = error_output(self.label, self.output)\n",
    "        for i in range(len(self.layers),0,-1):\n",
    "            i -= 1\n",
    "            self.layers[i].error = error\n",
    "            if i != max(self.layers.keys()):\n",
    "                to_pass = self.outputs_by_layer[i]\n",
    "                error = np.multiply( np.multiply( self.outputs_by_layer[i], (1-self.outputs_by_layer[i])),\n",
    "                                    error.dot(self.layers[i+1].weights.T))\n",
    "            #print(i,self.outputs_by_layer[i-1].T.shape, error.shape,)\n",
    "            if i == 0:\n",
    "                self.to_pass = self.in_data.copy()\n",
    "            else:\n",
    "                self.to_pass = self.outputs_by_layer[i-1].copy()\n",
    "            #print(i,self.to_pass.shape,error.shape,self.to_pass.T.dot(error).shape)\n",
    "            self.layers[i].grads_w = self.to_pass.T.dot(error)\n",
    "            self.layers[i].grads_b = error.sum(axis=0)\n",
    "            #print(i,self.layers[i].grads_w.shape, self.layers[i].grads_b.shape)\n",
    "\n",
    "    def update_weights(self, train_rate=0.05):\n",
    "        i=0\n",
    "        for ilayer in self.layers.values():\n",
    "            i+=1\n",
    "            #print(i, ilayer.weights.shape, ilayer.grads_w.shape)\n",
    "            ilayer.weights -= (ilayer.grads_w * train_rate)\n",
    "            ilayer.bias -= (ilayer.grads_b * train_rate)\n",
    "        \n",
    "\n",
    "    def train(self, iters, train_rate = 0.05):\n",
    "        self.old_error = (self.error.sum())\n",
    "        for i in range(iters):\n",
    "            self.backpropagate()\n",
    "            self.update_weights()\n",
    "            self.score_network()\n",
    "        self.new_error = self.error.sum()\n",
    "        print(self.new_error - self.old_error)\n",
    "        \n",
    "    def get_batched_network_output(self,batch):\n",
    "        data_in = self.in_data.copy()[batch]\n",
    "        for ilayer in self.layers.values():\n",
    "            data_in = ilayer.get_layer_output(data_in).T\n",
    "        return data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.210659352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmacdonald/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "t = Network()\n",
    "t.set_indata(g[:100], label[:100])\n",
    "\n",
    "t.assign_layer(15,'sigmoid', 128)\n",
    "#t.assign_layer(20,'sigmoid', 25)\n",
    "#t.assign_layer(15,'sigmoid', 20)\n",
    "t.assign_layer(12,'sigmoid',15)\n",
    "t.assign_layer(10,'softmax',12)\n",
    "\n",
    "t.score_network()\n",
    "print(t.error.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmacdonald/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.6624110736\n"
     ]
    }
   ],
   "source": [
    "t.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True, False,\n",
       "        True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 1451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.prediction == label[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
