{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "import h5py\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "global_variables = {\n",
    "    'epsilon':1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shuffle_index(array):\n",
    "    train_ix = np.random.choice(range(len(array)), replace=False, size=int(len(array)))\n",
    "    return train_ix\n",
    "    \n",
    "def get_indices(array, index):\n",
    "    return array[index]\n",
    "\n",
    "def shuffle(arrays):\n",
    "    indices = get_shuffle_index(arrays[0])\n",
    "    return [get_indices(array,indices) for array in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return (y_pred == y_true).mean()\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffled=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffled:\n",
    "        X, y = shuffle([X, y])\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_forward(X, W, b):\n",
    "    out = X @ W + b\n",
    "    for_backprop = (W, X)\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def fc_backward(dout, for_backprop):\n",
    "    W, h = for_backprop\n",
    "\n",
    "    dW = h.T @ dout\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dX = dout @ W.T\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "def softmax(X):\n",
    "    eX = np.exp((X.T - np.max(X, axis=1)).T)\n",
    "    return (eX.T / eX.sum(axis=1)).T\n",
    "\n",
    "def relu_forward(X):\n",
    "    out = np.maximum(X, 0)\n",
    "    for_backprop = X\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def relu_backward(dout, for_backprop):\n",
    "    dX = dout.copy()\n",
    "    dX[for_backprop <= 0] = 0\n",
    "    return dX\n",
    "\n",
    "\n",
    "def lrelu_forward(X, a=1e-3):\n",
    "    out = np.maximum(a * X, X)\n",
    "    for_backprop = (X, a)\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def lrelu_backward(dout, for_backprop):\n",
    "    X, a = for_backprop\n",
    "    dX = dout.copy()\n",
    "    dX[X < 0] *= a\n",
    "    return dX\n",
    "\n",
    "\n",
    "def sigmoid_forward(X):\n",
    "    out = np.sigmoid(X)\n",
    "    for_backprop = out\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def sigmoid_backward(dout, for_backprop):\n",
    "    return for_backprop * (1. - for_backprop) * dout\n",
    "\n",
    "\n",
    "def tanh_forward(X):\n",
    "    out = np.tanh(X)\n",
    "    for_backprop = out\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def tanh_backward(dout, for_backprop):\n",
    "    dX = (1 - for_backprop**2) * dout\n",
    "    return dX\n",
    "\n",
    "\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    for_backprop = u\n",
    "    return out, for_backprop\n",
    "\n",
    "\n",
    "def dropout_backward(dout, for_backprop):\n",
    "    dX = dout * for_backprop\n",
    "    return dX\n",
    "\n",
    "\n",
    "def bn_forward(X, gamma, beta, for_backprop, momentum=.9, train=True):\n",
    "    rmean, rvar = for_backprop\n",
    "\n",
    "    if train:\n",
    "        mu = np.mean(X, axis=0)\n",
    "        var = np.var(X, axis=0)\n",
    "        \n",
    "        #print(mu.shape, rmean.shape, var.shape, rvar.shape)\n",
    "\n",
    "        X_norm = (X - mu) / np.sqrt(var + global_variables['epsilon'])\n",
    "        out = gamma * X_norm + beta\n",
    "\n",
    "        for_backprop = (X, X_norm, mu, var, gamma, beta)\n",
    "\n",
    "        rmean = exp_running_avg(rmean, mu, momentum)\n",
    "        rvar = exp_running_avg(rvar, var, momentum)\n",
    "    else:\n",
    "        X_norm = (X - rmean) / np.sqrt(rvar + global_variables['epsilon'])\n",
    "        out = gamma * X_norm + beta\n",
    "        for_backprop = None\n",
    "\n",
    "    return out, for_backprop, rmean, rvar\n",
    "\n",
    "def exp_running_avg(running, new, gamma=.9):\n",
    "    return gamma * running + (1. - gamma) * new\n",
    "\n",
    "def bn_backward(dout, for_backprop):\n",
    "    X, X_norm, mu, var, gamma, beta = for_backprop\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    X_mu = X - mu\n",
    "    std_inv = 1. / np.sqrt(var + global_variables['epsilon'])\n",
    "\n",
    "    dX_norm = dout * gamma\n",
    "    dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n",
    "    dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "    dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n",
    "    dgamma = np.sum(dout * X_norm, axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "    return dX, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cel(y_pred, y_train):\n",
    "    m = y_pred.shape[0]\n",
    "    prob = softmax(y_pred)\n",
    "    log_like = -np.log(prob[range(m), y_train])\n",
    "    return np.sum(log_like) / m\n",
    "\n",
    "\n",
    "def d_cel(y_pred, y_train):\n",
    "    m = y_pred.shape[0]\n",
    "    grad_y = softmax(y_pred)\n",
    "    grad_y[range(m), y_train] -= 1.\n",
    "    grad_y /= m\n",
    "    return grad_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet:\n",
    "        \n",
    "    def __init__(self, D, C, H, lam=1e-3, p_dropout=.8, nonlin='relu'):\n",
    "        self._init_model(D, C, H)\n",
    "\n",
    "        self.lam = lam\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mode = 'classification'\n",
    "\n",
    "    def train_step(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Single training step over minibatch: forward, loss, backprop\n",
    "        \"\"\"\n",
    "        y_pred, cache = self.forward(X_train, train=True)\n",
    "        loss = cel(y_pred, y_train)\n",
    "        grad = self.backpropagation(y_pred, y_train, cache)\n",
    "\n",
    "        return grad, loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        score, _ = self.forward(X, False)\n",
    "        return softmax(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def forward(self, X, train=False):\n",
    "        gamma1, gamma2 = self.model['gamma1'], self.model['gamma2']\n",
    "        beta1, beta2 = self.model['beta1'], self.model['beta2']\n",
    "\n",
    "        u1, u2 = None, None\n",
    "        bn1_cache, bn2_cache = None, None\n",
    "\n",
    "        # First layer\n",
    "        h1, h1_cache = fc_forward(X, self.model['W1'], self.model['b1'])\n",
    "        bn1_cache = (self.bn_caches['bn1_mean'], self.bn_caches['bn1_var'])\n",
    "        h1, bn1_cache, run_mean, run_var = bn_forward(h1, gamma1, beta1, bn1_cache, train=train)\n",
    "        h1, nl_cache1 = relu_forward(h1)\n",
    "\n",
    "        self.bn_caches['bn1_mean'], self.bn_caches['bn1_var'] = run_mean, run_var\n",
    "\n",
    "        if train:\n",
    "            h1, u1 = dropout_forward(h1, self.p_dropout)\n",
    "\n",
    "        # Second layer\n",
    "        h2, h2_cache = fc_forward(h1, self.model['W2'], self.model['b2'])\n",
    "        bn2_cache = (self.bn_caches['bn2_mean'], self.bn_caches['bn2_var'])\n",
    "        h2, bn2_cache, run_mean, run_var = bn_forward(h2, gamma2, beta2, bn2_cache, train=train)\n",
    "        h2, nl_cache2 = relu_forward(h2)\n",
    "\n",
    "        self.bn_caches['bn2_mean'], self.bn_caches['bn2_var'] = run_mean, run_var\n",
    "\n",
    "        if train:\n",
    "            h2, u2 = dropout_forward(h2, self.p_dropout)\n",
    "\n",
    "        # Third layer\n",
    "        score, score_cache = fc_forward(h2, self.model['W3'], self.model['b3'])\n",
    "\n",
    "        cache = (X, h1_cache, h2_cache, score_cache, nl_cache1, nl_cache2, u1, u2, bn1_cache, bn2_cache)\n",
    "\n",
    "        return score, cache\n",
    "\n",
    "    def backpropagation(self, y_pred, y_train, cache):\n",
    "        X, h1_cache, h2_cache, score_cache, nl_cache1, nl_cache2, u1, u2, bn1_cache, bn2_cache = cache\n",
    "\n",
    "        # Hidden layer 3\n",
    "        to_pass = d_cel(y_pred, y_train)\n",
    "        to_pass, d_weights_3, d_bias_3 = fc_backward(to_pass, score_cache)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        to_pass = relu_backward(to_pass, nl_cache2)\n",
    "        to_pass = dropout_backward(to_pass, u2)\n",
    "        to_pass, dgamma2, dbeta2 = bn_backward(to_pass, bn2_cache)\n",
    "        to_pass, d_weights_2, d_bias_2 = fc_backward(to_pass, h2_cache)\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        to_pass = relu_backward(to_pass, nl_cache1)\n",
    "        to_pass = dropout_backward(to_pass, u1)\n",
    "        to_pass, dgamma1, dbeta1 = bn_backward(to_pass, bn1_cache)\n",
    "        _, d_weights_1, d_bias_1 = fc_backward(to_pass, h1_cache)\n",
    "\n",
    "        gradients = dict(\n",
    "            W1=d_weights_1, W2=d_weights_2, W3=d_weights_3, \n",
    "            b1=d_bias_1, b2=d_bias_2, b3=d_bias_3, \n",
    "            gamma1=dgamma1,gamma2=dgamma2, beta1=dbeta1, beta2=dbeta2\n",
    "        )\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def _init_model(self, D, C, H):\n",
    "        self.model = dict(\n",
    "            W1=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            W2=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            W3=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((1, H)),\n",
    "            b2=np.zeros((1, H)),\n",
    "            b3=np.zeros((1, C)),\n",
    "            gamma1=np.ones((1, H)),\n",
    "            gamma2=np.ones((1, H)),\n",
    "            beta1=np.zeros((1, H)),\n",
    "            beta2=np.zeros((1, H))\n",
    "        )\n",
    "\n",
    "        self.bn_caches = dict(\n",
    "            bn1_mean=np.zeros((1, H)),\n",
    "            bn2_mean=np.zeros((1, H)),\n",
    "            bn1_var=np.zeros((1, H)),\n",
    "            bn2_var=np.zeros((1, H))\n",
    "        )\n",
    "        \n",
    "    def momentum(self, X_train, y_train, test=None, train_rate=1e-3, mb_size=256, iterations=2000, p_iter=100):\n",
    "        velocity = {k: np.zeros_like(v) for k, v in self.model.items()}\n",
    "        gamma = .9\n",
    "\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size)\n",
    "\n",
    "        if test:\n",
    "            X_val, y_val = test\n",
    "        else:\n",
    "            X_val, y_val = X_train, y_train\n",
    "\n",
    "        for iteration in range(1, iterations + 1):\n",
    "            if iteration == iterations:\n",
    "                print(iteration)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "            gradients, loss = self.train_step(X_mini, y_mini)\n",
    "\n",
    "            if iteration % p_iter == 0:\n",
    "                val_acc = accuracy(y_val, self.predict(X_val))\n",
    "                print('Iter-{} loss: {:.4f} validation: {:4f}'.format(iteration, loss, val_acc))\n",
    "\n",
    "            for layer in gradients:\n",
    "                velocity[layer] = gamma * velocity[layer] + train_rate * gradients[layer]\n",
    "                self.model[layer] -= velocity[layer]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "t = FeedForwardNet(128, 10, 200)\n",
    "X_train = data[:2000]\n",
    "Y_train = label[:2000]\n",
    "X_val = data[2000:2500]\n",
    "Y_val = label[2000:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-200 loss: 3.1180 validation: 0.588000\n",
      "Iter-400 loss: 2.4682 validation: 0.592000\n",
      "Iter-600 loss: 2.3061 validation: 0.588000\n",
      "Iter-800 loss: 2.3345 validation: 0.606000\n",
      "1000\n",
      "Iter-1000 loss: 2.3921 validation: 0.564000\n",
      "CPU times: user 19 s, sys: 1.92 s, total: 20.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D = 128\n",
    "C = 10\n",
    "H = 200\n",
    "lam = 1e-3\n",
    "p_dropout=0.05\n",
    "loss='cross_ent'\n",
    "nonlin='relu'\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "t = FeedForwardNet(D, C, H=500, lam=lam, p_dropout=p_dropout, nonlin=nonlin)\n",
    "t = t.momentum(X_train, Y_train, test=(X_val, Y_val), mb_size=100, iterations=1000, p_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
