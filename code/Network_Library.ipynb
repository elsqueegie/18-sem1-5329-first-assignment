{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=1)\n",
    "import h5py\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('../data/Assignment-1-Dataset/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('../data/Assignment-1-Dataset/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "data = data.copy()\n",
    "#data = np.c_[data, np.ones(len(data))]\n",
    "\n",
    "g = data.copy()\n",
    "\n",
    "weights = (np.random.rand(g.shape[1]) - 0.5)/100\n",
    "alphas = (np.random.rand(g.shape[1]) - 0.5)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_constant(data):\n",
    "    return np.c_[data, np.ones(len(data))]\n",
    "\n",
    "def relu(matrix):\n",
    "    # activation function that gives x if x > 0 else 0\n",
    "    return np.clip(matrix, 0, None)\n",
    "\n",
    "def leaky_relu(matrix, alpha=0.005):\n",
    "    # activation function that draws on ReLU but has a slight gradient for x < 0\n",
    "    matrix[matrix<0] = matrix[matrix<0] * alpha\n",
    "    return matrix\n",
    "\n",
    "def sigmoid(column):\n",
    "    # activation function that returns a value between 0 and 1, good for probabilities\n",
    "    return 1/(1+np.exp(-column))\n",
    "\n",
    "def tanh(column):\n",
    "    # Activation function that returns a value between -1 and 1\n",
    "    # problems occurred with large negatives when applying (1 - np.exp(-column)) / (1 + np.exp(-column)) \n",
    "    # using numpy equivalent instead\n",
    "    return np.tanh(column)\n",
    "\n",
    "def softmax(layer_output):\n",
    "    j = np.exp(layer_output)\n",
    "    j /= j.sum(axis=1).reshape(len(j),1)\n",
    "    return j\n",
    "\n",
    "#def node_softmax(in_data, weights):\n",
    "#    # Multiply each feature (including constant) by its weight then sum the result\n",
    "#    in_data = in_data.copy()\n",
    "#    for d in range(in_data.shape[1]):\n",
    "#        in_data[:,d] = np.exp(weights[d] * in_data[:,d])\n",
    "#    in_data = in_data / in_data.sum(axis=1)\n",
    "#    return in_data\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return np.multiply( x, (1-x))\n",
    "    \n",
    "def derivative_tanh(x):\n",
    "    return 1 - np.multiply(x, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return (x > 0) - 0.0\n",
    "\n",
    "def derivative_leaky_relu(x, alpha=0.005):\n",
    "    #returns the gradient of a leaky_relu at point x\n",
    "    to_ret = grad_relu(x)\n",
    "    to_ret[to_ret<=0] = alpha\n",
    "    return to_ret\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    #returns the gradient of a sigmoid at point x\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def grad_tanh(x):\n",
    "    #returns the gradient of a tanh at point x\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def grad_relu(x):\n",
    "    #returns the gradient of a relu at point x\n",
    "    return (np.array(x) > 0) - 0.0\n",
    "\n",
    "def grad_leaky_relu(x, alpha=0.005):\n",
    "    #returns the gradient of a leaky_relu at point x\n",
    "    to_ret = np.array(grad_relu(x))\n",
    "    to_ret[to_ret<=0] = alpha\n",
    "    return to_ret\n",
    "    \n",
    "def grad_softmax(prediction_list, iclass):\n",
    "    prediction_list /= np.max(prediction_list,axis=1, keepdims=True)\n",
    "    j = np.exp(prediction_list)\n",
    "    j =  j/j.sum()\n",
    "    yhat = j[iclass]\n",
    "    return yhat - 1\n",
    "    \n",
    "def get_gradient(activation_function):\n",
    "    gradient_dic = {\n",
    "        'relu':grad_relu,\n",
    "        'leaky_relu':grad_leaky_relu,\n",
    "        'tanh':grad_tanh,\n",
    "        'sigmoid':grad_sigmoid,\n",
    "        'softmax':grad_softmax\n",
    "    }\n",
    "    return gradient_dic[activation_function]\n",
    "\n",
    "get_derivative = {\n",
    "    'sigmoid':derivative_sigmoid,\n",
    "    'tanh':derivative_tanh,\n",
    "    'relu':derivative_relu,\n",
    "    'leaky_relu':derivative_leaky_relu,\n",
    "}\n",
    "\n",
    "def activate(in_data, kind='relu'):\n",
    "    # Apply an activation function to a node's output\n",
    "    actionary = {\n",
    "        'relu':relu,\n",
    "        'leaky_relu':leaky_relu,\n",
    "        'sigmoid':sigmoid,\n",
    "        'tanh':tanh,\n",
    "        'softmax':softmax\n",
    "    }\n",
    "    return actionary[kind](in_data)\n",
    "\n",
    "def compute_softmax_scores(layer_output):\n",
    "    j = np.exp(layer_output)\n",
    "    j /= j.sum(axis=1).reshape(len(j),1)\n",
    "    return j\n",
    "    \n",
    "def compute_cross_entropy_loss(yhat,label):\n",
    "    return 0 - np.log(yhat[:,label])[0]\n",
    "\n",
    "def get_cross_entropy_grads(output,label):\n",
    "    n_instances = len(label)\n",
    "    gradients_matrix = compute_softmax_scores(output)\n",
    "    gradients_matrix[range(n_instances),label] -= 1\n",
    "    gradients_matrix /= n_instances\n",
    "    return gradients_matrix\n",
    "\n",
    "\n",
    "def node_mult(in_data, weights, softmax=False):\n",
    "    # Multiply each feature (including constant) by its weight then sum the result\n",
    "    in_data = add_constant(in_data)\n",
    "    if in_data.shape[1] != weights.shape[1]:\n",
    "        raise ValueError(\"Input matrix doesn't match weight vector: {} weights and {} features\".format(\n",
    "            len(weights),in_data.shape[1])\n",
    "        )\n",
    "    for d in range(in_data.shape[1]):\n",
    "        in_data[:,d] = weights[d] * in_data[:,d]\n",
    "    if softmax:\n",
    "        in_data[:,:-1] += in_data[:,-1:]\n",
    "        in_data = in_data[:,:-1]\n",
    "        in_data = np.exp(in_data)\n",
    "        return in_data/in_data.sum(axis=1).reshape((len(in_data),1))\n",
    "    else:\n",
    "        return in_data.sum(axis=1)\n",
    "    \n",
    "def layer_mult(in_data, weights, bias, activation_func='sigmoid'):\n",
    "    # Multiply each feature (including constant) by its weight then sum the result\n",
    "    #in_data = add_constant(in_data)\n",
    "    if in_data.shape[1] != weights.shape[0]:\n",
    "        raise ValueError(\"Input matrix doesn't match weight vector: {} weights and {} features\".format(\n",
    "            len(weights),in_data.shape[1])\n",
    "        )\n",
    "    if activation_func=='softmax':\n",
    "        layer_out = in_data.dot(weights) + bias\n",
    "        return softmax(layer_out)\n",
    "    else:\n",
    "        layer_out = in_data.dot(weights) + bias\n",
    "        return activate(layer_out, kind=activation_func).T\n",
    "    \n",
    "def matricise_label(label,output):\n",
    "    T = np.zeros_like(output)\n",
    "    T[range(len(T)),label] += 1\n",
    "    return T\n",
    "\n",
    "def get_cost(label,output):\n",
    "    label = matricise_label(label,output)\n",
    "    return -np.multiply(label,np.log(output)).sum()\n",
    "\n",
    "def error_output(label,output):\n",
    "    label = matricise_label(label,output)\n",
    "    return output - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, n_nodes, activation, n_inputs):\n",
    "        self.activation = activation\n",
    "        afunc = self.activation\n",
    "        self.weights = np.random.random((n_inputs,n_nodes))\n",
    "        self.bias = np.random.randn(n_nodes)\n",
    "        \n",
    "    def get_layer_output(self, df_in):\n",
    "        self.in_data = df_in\n",
    "        self.output = layer_mult(df_in, self.weights, self.bias, activation_func=self.activation)\n",
    "        return self.output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = {}\n",
    "        self.in_data = None\n",
    "        self.n_layers = 0\n",
    "        \n",
    "    def set_indata(self, in_data, label):\n",
    "        self.in_data = in_data\n",
    "        self.label = label\n",
    "        self.in_features = in_data.shape[1]\n",
    "        self.to_pass = self.in_data\n",
    "    \n",
    "    def assign_layer(self, n_nodes, activation, n_inputs):\n",
    "        self.layers[self.n_layers] = Layer(n_nodes, activation, n_inputs)\n",
    "        #self.layers[self.n_layers].set_input(self.to_pass)\n",
    "        self.n_layers += 1\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        self.outputs_by_layer = []\n",
    "        data_in = self.in_data.copy()\n",
    "        for ilayer in self.layers.values():\n",
    "            #print(ilayer.activation)\n",
    "            self.ilayer = ilayer\n",
    "            data_in = ilayer.get_layer_output(data_in).T\n",
    "            self.outputs_by_layer.append(data_in.copy())\n",
    "        self.output = data_in.reshape(data_in.shape[:2]).T\n",
    "        return self.outputs_by_layer\n",
    "\n",
    "    \n",
    "    def score_network(self):\n",
    "        self.feed_forward()\n",
    "        self.prediction = np.argmax(t.output, axis=1).reshape(len(t.label))\n",
    "        self.error = compute_cross_entropy_loss(self.output, self.label)\n",
    "\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return np.mean([compute_cross_entropy_loss(\n",
    "                compute_softmax_score(self.output[i],self.label[i])\n",
    "        ) for i in range(len(self.label))])\n",
    "    \n",
    "    def get_batched_loss(self,batch):\n",
    "        loss = [compute_cross_entropy_loss(compute_softmax_score(self.output[i], self.label[i])) for i in batch]\n",
    "        return loss\n",
    "    \n",
    "    def get_batch(self, frac=0.05):\n",
    "        return np.random.choice(range(len(self.in_data)), replace=False, size=int(len(self.in_data)*frac))\n",
    "\n",
    "    \n",
    "    def backpropagate(self):\n",
    "        cost = get_cost(self.label, self.output)\n",
    "        error = error_output(self.label, self.output)\n",
    "        for i in range(len(self.layers),0,-1):\n",
    "            i -= 1\n",
    "            self.layers[i].error = error\n",
    "            if i != max(self.layers.keys()):\n",
    "                to_pass = self.outputs_by_layer[i]\n",
    "                error = np.multiply( get_derivative[self.layers[i].activation](self.outputs_by_layer[i]),\n",
    "                                    error.dot(self.layers[i+1].weights.T))\n",
    "            #print(i,self.outputs_by_layer[i-1].T.shape, error.shape,)\n",
    "            if i == 0:\n",
    "                self.to_pass = self.in_data.copy()\n",
    "            else:\n",
    "                self.to_pass = self.outputs_by_layer[i-1].copy()\n",
    "            #print(i,self.to_pass.shape,error.shape,self.to_pass.T.dot(error).shape)\n",
    "            self.layers[i].grads_w = self.to_pass.T.dot(error)\n",
    "            self.layers[i].grads_b = error.sum(axis=0)\n",
    "            #print(i,self.layers[i].grads_w.shape, self.layers[i].grads_b.shape)\n",
    "\n",
    "    def update_weights(self, train_rate=0.05):\n",
    "        i=0\n",
    "        for ilayer in self.layers.values():\n",
    "            i+=1\n",
    "            #print(i, ilayer.weights.shape, ilayer.grads_w.shape)\n",
    "            ilayer.weights -= (ilayer.grads_w * train_rate)\n",
    "            ilayer.bias -= (ilayer.grads_b * train_rate)\n",
    "        \n",
    "\n",
    "    def train(self, iters, train_rate = 0.05):\n",
    "        self.old_error = get_cost(self.label, self.output)\n",
    "        for i in range(iters):\n",
    "            self.backpropagate()\n",
    "            self.update_weights()\n",
    "            self.score_network()\n",
    "        self.new_error = get_cost(self.label, self.output)\n",
    "        print(self.new_error - self.old_error)\n",
    "        \n",
    "    def get_batched_network_output(self,batch):\n",
    "        data_in = self.in_data.copy()[batch]\n",
    "        for ilayer in self.layers.values():\n",
    "            data_in = ilayer.get_layer_output(data_in).T\n",
    "        return data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266.037735395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmacdonald/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "t = Network()\n",
    "t.set_indata(g[:100], label[:100])\n",
    "\n",
    "t.assign_layer(25,'sigmoid', 128)\n",
    "#t.assign_layer(20,'tanh', 25)\n",
    "#t.assign_layer(15,'leaky_relu', 20)\n",
    "#t.assign_layer(12,'sigmoid',15)\n",
    "t.assign_layer(10,'softmax',25)\n",
    "\n",
    "t.score_network()\n",
    "print(t.error.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmacdonald/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-338.293675234\n"
     ]
    }
   ],
   "source": [
    "t.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t.prediction == label[:100]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 9),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (3, 3),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (7, 7),\n",
       " (2, 2),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (0, 0),\n",
       " (9, 9),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (7, 7),\n",
       " (9, 9),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (6, 6),\n",
       " (0, 4),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (4, 4),\n",
       " (8, 8),\n",
       " (4, 4),\n",
       " (3, 3),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (3, 3),\n",
       " (6, 6),\n",
       " (0, 6),\n",
       " (0, 0),\n",
       " (8, 8),\n",
       " (5, 5),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 6),\n",
       " (6, 6),\n",
       " (9, 7),\n",
       " (9, 9),\n",
       " (5, 5),\n",
       " (9, 9),\n",
       " (2, 2),\n",
       " (7, 7),\n",
       " (3, 3),\n",
       " (0, 0),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (7, 7),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (0, 6),\n",
       " (6, 6),\n",
       " (8, 8),\n",
       " (3, 3),\n",
       " (1, 3),\n",
       " (5, 5),\n",
       " (0, 0),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (4, 4),\n",
       " (1, 1),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (6, 6),\n",
       " (3, 3),\n",
       " (1, 1),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (0, 6),\n",
       " (1, 1),\n",
       " (9, 9),\n",
       " (1, 1),\n",
       " (3, 3),\n",
       " (5, 5),\n",
       " (7, 7),\n",
       " (9, 9),\n",
       " (5, 7),\n",
       " (1, 1),\n",
       " (7, 7),\n",
       " (9, 9),\n",
       " (9, 9),\n",
       " (9, 9),\n",
       " (3, 3),\n",
       " (2, 2),\n",
       " (9, 9),\n",
       " (3, 3),\n",
       " (6, 6),\n",
       " (4, 4),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (8, 8)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(t.prediction, label[:100]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
